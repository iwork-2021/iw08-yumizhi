{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset. It contains translations from English to Spanish, so swap the order of the phrases. Also add `\\t` and `\\n` as the start and stop tokens in the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"\\t\"\n",
    "stop_token = \"\\n\"\n",
    "\n",
    "with open(\"data/spa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    samples = f.read().split(\"\\n\")\n",
    "\n",
    "samples = [sample.strip().split(\"\\t\")\n",
    "           for sample in samples if len(sample.strip()) > 0]\n",
    "\n",
    "samples = [(es, start_token + en + stop_token)\n",
    "           for en, es in samples if len(es) < 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99423"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ve.', '\\tGo.\\n'), ('Vete.', '\\tGo.\\n'), ('Vaya.', '\\tGo.\\n'), ('Váyase.', '\\tGo.\\n'), ('Hola.', '\\tHi.\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(samples[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_samples, valid_samples = train_test_split(samples, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79538"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the training vocabulary. Those are the only tokens you can trust the model will know how to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 101\n",
      "Output vocab size: 87\n"
     ]
    }
   ],
   "source": [
    "in_vocab = set()\n",
    "out_vocab = set()\n",
    "\n",
    "for in_seq, out_seq in train_samples:\n",
    "    in_vocab.update(in_seq)\n",
    "    out_vocab.update(out_seq)\n",
    "    \n",
    "in_vocab_size = len(in_vocab)\n",
    "out_vocab_size = len(out_vocab)\n",
    "print(\"Input vocab size:\", in_vocab_size)\n",
    "print(\"Output vocab size:\", out_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '°', 'º', '»', '¿', 'Á', 'É', 'Ó', 'Ú', 'á', 'è', 'é', 'í', 'ñ', 'ó', 'ö', 'ú', 'ü', 'ś', 'с', '—', '€']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(in_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', ' ', '!', '\"', '$', '%', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', 'á', 'ã', 'è', 'é', 'ö', '‘', '’', '₂', '€']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(out_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through validation set and remove any tokens not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_samples = []\n",
    "for in_seq, out_seq in valid_samples:\n",
    "    tmp_in_seq = [c for c in in_seq if c in in_vocab]\n",
    "    tmp_out_seq = [c for c in out_seq if c in out_vocab]\n",
    "\n",
    "    tmp_samples.append((\"\".join(tmp_in_seq), \"\".join(tmp_out_seq)))\n",
    "    \n",
    "valid_samples = tmp_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build sequence-to-sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Input, LSTM, Masking\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:29:22.204504: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "encoder_in = Input(shape=(None, in_vocab_size), name=\"encoder_in\")\n",
    "encoder_mask = Masking(name=\"encoder_mask\")(encoder_in)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, recurrent_dropout=0.3, name=\"encoder_lstm\")\n",
    "_, encoder_h, encoder_c = encoder_lstm(encoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_in = Input(shape=(None, out_vocab_size), name=\"decoder_in\")\n",
    "\n",
    "decoder_mask = Masking(name=\"decoder_mask\")(decoder_in)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                    dropout=0.2, recurrent_dropout=0.3, name=\"decoder_lstm\")\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_mask, initial_state=[encoder_h, encoder_c])\n",
    "decoder_dense = Dense(out_vocab_size, activation=\"softmax\", name=\"decoder_out\")\n",
    "decoder_out = decoder_dense(decoder_lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = Model([encoder_in, decoder_in], decoder_out)\n",
    "seq2seq_model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_in (InputLayer)        [(None, None, 101)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder_in (InputLayer)        [(None, None, 87)]   0           []                               \n",
      "                                                                                                  \n",
      " encoder_mask (Masking)         (None, None, 101)    0           ['encoder_in[0][0]']             \n",
      "                                                                                                  \n",
      " decoder_mask (Masking)         (None, None, 87)     0           ['decoder_in[0][0]']             \n",
      "                                                                                                  \n",
      " encoder_lstm (LSTM)            [(None, 256),        366592      ['encoder_mask[0][0]']           \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  352256      ['decoder_mask[0][0]',           \n",
      "                                 (None, 256),                     'encoder_lstm[0][1]',           \n",
      "                                 (None, 256)]                     'encoder_lstm[0][2]']           \n",
      "                                                                                                  \n",
      " decoder_out (Dense)            (None, None, 87)     22359       ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 741,207\n",
      "Trainable params: 741,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create maps to convert characters to and from ints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_token2int = {token : i for i, token in enumerate(sorted(in_vocab))}\n",
    "out_token2int = {token : i for i, token in enumerate(sorted(out_vocab))}\n",
    "out_int2token = {i : token for (token, i) in out_token2int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions for one-hot encoding sequences for use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_batch_storage(batch_size, in_seq_len, out_seq_len):\n",
    "    \n",
    "    enc_in_seqs = np.zeros(\n",
    "        (batch_size, in_seq_len, in_vocab_size),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    dec_in_seqs = np.zeros(\n",
    "        (batch_size, out_seq_len, out_vocab_size),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    dec_out_seqs = np.zeros(\n",
    "        (batch_size, out_seq_len, out_vocab_size),\n",
    "        dtype=np.float32)\n",
    "        \n",
    "    return enc_in_seqs, dec_in_seqs, dec_out_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(samples):\n",
    "    batch_size = len(samples)\n",
    "    max_in_length = max([len(seq) for seq, _ in samples])\n",
    "    max_out_length = max([len(seq) for _, seq in samples])\n",
    "\n",
    "    enc_in_seqs, dec_in_seqs, dec_out_seqs = make_batch_storage(\n",
    "        batch_size, max_in_length, max_out_length)\n",
    "    \n",
    "    for i, (in_seq, out_seq) in enumerate(samples):\n",
    "        for time_step, token in enumerate(in_seq):\n",
    "            enc_in_seqs[i, time_step, in_token2int[token]] = 1\n",
    "\n",
    "        for time_step, token in enumerate(out_seq):\n",
    "            dec_in_seqs[i, time_step, out_token2int[token]] = 1\n",
    "\n",
    "        for time_step, token in enumerate(out_seq[1:]):\n",
    "            dec_out_seqs[i, time_step, out_token2int[token]] = 1\n",
    "            \n",
    "    return enc_in_seqs, dec_in_seqs, dec_out_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_util import Seq2SeqBatchGenerator\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = Seq2SeqBatchGenerator(train_samples, batch_size, encode_batch)\n",
    "valid_generator = Seq2SeqBatchGenerator(valid_samples, batch_size, encode_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ParagonLight/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 244/1247 [====>.........................] - ETA: 9:10 - loss: 1.7837"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g2/d875dk7s4xq8j03j59rzx0w40000gn/T/ipykernel_98850/292808357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m seq2seq_model.fit_generator(train_generator, epochs=500,\n\u001b[1;32m      6\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                             callbacks=[early_stopping])\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2028\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "seq2seq_model.fit_generator(train_generator, epochs=500,\n",
    "                            validation_data=valid_generator,\n",
    "                            callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoder/decoder models for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_encoder = Model(encoder_in, [encoder_h, encoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_in (InputLayer)     [(None, None, 101)]       0         \n",
      "                                                                 \n",
      " encoder_mask (Masking)      (None, None, 101)         0         \n",
      "                                                                 \n",
      " encoder_lstm (LSTM)         [(None, 256),             366592    \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 366,592\n",
      "Trainable params: 366,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dec_h_in = Input(shape=(latent_dim,), name=\"decoder_h_in\")\n",
    "inf_dec_c_in = Input(shape=(latent_dim,), name=\"decoder_c_in\")\n",
    "\n",
    "inf_dec_lstm_out, inf_dec_h_out, inf_dec_c_out = decoder_lstm(\n",
    "    decoder_in, initial_state=[inf_dec_h_in, inf_dec_c_in])\n",
    "\n",
    "inf_dec_out = decoder_dense(inf_dec_lstm_out)\n",
    "\n",
    "inf_decoder = Model(\n",
    "    [decoder_in, inf_dec_h_in, inf_dec_c_in],\n",
    "    [inf_dec_out, inf_dec_h_out, inf_dec_c_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_in (InputLayer)        [(None, None, 87)]   0           []                               \n",
      "                                                                                                  \n",
      " decoder_h_in (InputLayer)      [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_c_in (InputLayer)      [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 256),  352256      ['decoder_in[0][0]',             \n",
      "                                 (None, 256),                     'decoder_h_in[0][0]',           \n",
      "                                 (None, 256)]                     'decoder_c_in[0][0]']           \n",
      "                                                                                                  \n",
      " decoder_out (Dense)            (None, None, 87)     22359       ['decoder_lstm[1][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 374,615\n",
      "Trainable params: 374,615\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inf_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test trained model on the first 100 samples from both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max output length:  87\n"
     ]
    }
   ],
   "source": [
    "max_out_seq_len = max([len(seq) for _, seq in samples])\n",
    "print(\"Max output length: \", max_out_seq_len)\n",
    "\n",
    "start_token_idx = out_token2int[start_token]\n",
    "stop_token_idx = out_token2int[stop_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(one_hot_seq, encoder, decoder):\n",
    "    encoding = encoder.predict(one_hot_seq)\n",
    "\n",
    "    decoder_in = np.zeros((1, 1, out_vocab_size), dtype=np.float32)\n",
    "\n",
    "    translated_text = ''\n",
    "    done_decoding = False\n",
    "    decoded_idx = start_token_idx\n",
    "    while not done_decoding:\n",
    "        decoder_in[0, 0, decoded_idx] = 1\n",
    "        decoding, h, c = decoder.predict([decoder_in] + encoding)\n",
    "        encoding = [h, c]\n",
    "        decoder_in[0, 0, decoded_idx] = 0\n",
    "\n",
    "        decoded_idx = np.argmax(decoding[0, -1, :])\n",
    "        \n",
    "        if decoded_idx == stop_token_idx:\n",
    "            done_decoding = True\n",
    "        else:\n",
    "            translated_text += out_int2token[decoded_idx]\n",
    "\n",
    "        if len(translated_text) >= max_out_seq_len:\n",
    "            done_decoding = True\n",
    "            \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Input sentence: A todos nos gusta montar en bici.\n",
      "Dataset translation: \tWe all like cycling.\n",
      "\n",
      "Model output: öI3V!XX€€’hhhhFF666  4, , aããfff6’,h.iios!\n",
      "-----------------------------------------\n",
      "Input sentence: Tom se rió de todos los chistes de Mary.\n",
      "Dataset translation: \tTom laughed at all of Mary's jokes.\n",
      "\n",
      "Model output: Xy8E1!c!écéTè’33U‘QKK0Km:0MX€€€’hhhhF6666  4, , aããfff6’,h.iisiámm!m//’’9N..llL9\n",
      "-----------------------------------------\n",
      "Input sentence: Tom es un asqueroso.\n",
      "Dataset translation: \tTom is a creep.\n",
      "\n",
      "Model output: XXa-//’/’9.é2Ba/.’érPö\tXXXyyy81€€€’hhhhFF666  4, , aããfff6’,h.iios!\n",
      "-----------------------------------------\n",
      "Input sentence: ¿Cuál es tu meta en la vida?\n",
      "Dataset translation: \tWhat's your aim in life?\n",
      "\n",
      "Model output: vv?OuuuttttttXddzdzsádáZffl9TOAIIKK0KKm:0MX€€’hhhh.ie.QéBa\"KK:0\"K0.\"KK:0\"KK:0\"K:0\"KK:0\"\n",
      "-----------------------------------------\n",
      "Input sentence: Ella le escucha, aunque nadie más lo haga.\n",
      "Dataset translation: \tShe listens to him even though no one else does.\n",
      "\n",
      "Model output: rtXdzZ0hhFFF₂FFö-₂ád,l9Rj9OYLLáp?OAocoouutttttdzdz0hhFFF₂F₂F-ddáplll99RL9\n",
      "-----------------------------------------\n",
      "Input sentence: Ganar o perder no es la cuestión.\n",
      "Dataset translation: \tIt doesn't matter whether you win or not.\n",
      "\n",
      "Model output: öXXXyy8€€’hhhhFF666ö,  , aããfff66’,h.iiA!écoL6666 ö277DD  a\"sRs1XX€€€’hhhhFF666  4, , a\n",
      "-----------------------------------------\n",
      "Input sentence: Es un privilegio conocerte.\n",
      "Dataset translation: \tIt is a privilege to meet you.\n",
      "\n",
      "Model output: öH‘0é-T₂gF₂FF₂FFö-'ddsEFFEFö'dsEFFdEFEFö'dsEFFdEFEFö'dsEFFdEFEFö'dsEFFdEFEFö'dsEFFdEFEF\n",
      "-----------------------------------------\n",
      "Input sentence: Disculpe, se me han caído los palillos.\n",
      "Dataset translation: \tExcuse me, I dropped a chopstick.\n",
      "\n",
      "Model output: XXa-//’/’9.é2Ba/.’érPö\tXXXyyy81€€€’hhhhFF666  4, , aããfff6’,h.iios!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g2/d875dk7s4xq8j03j59rzx0w40000gn/T/ipykernel_98850/1999772420.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseq2seq_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslate_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/OneDrive/iOS智能开发/资料/Machine_Learning_by_Tutorials_v2.4.0/15-seq2seq/notebooks/seq2seq_util.py\u001b[0m in \u001b[0;36mtest_predictions\u001b[0;34m(samples, encoder, decoder, batch_encoder, sequence_decoder)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mencoded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g2/d875dk7s4xq8j03j59rzx0w40000gn/T/ipykernel_98850/1980705846.py\u001b[0m in \u001b[0;36mtranslate_sequence\u001b[0;34m(one_hot_seq, encoder, decoder)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone_decoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdecoder_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdecoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_in\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdecoder_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mprefetch\u001b[0;34m(self, buffer_size, name)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mDEBUG_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPrefetchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, buffer_size, slack_period, name)\u001b[0m\n\u001b[1;32m   5752\u001b[0m       \u001b[0mbuffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5753\u001b[0m     self._buffer_size = ops.convert_to_tensor(\n\u001b[0;32m-> 5754\u001b[0;31m         buffer_size, dtype=dtypes.int64, name=\"buffer_size\")\n\u001b[0m\u001b[1;32m   5755\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1628\u001b[0m           \u001b[0;34m\"%sConversion function %r for type %s returned non-Tensor: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m           (_error_prefix(name), conversion_func, base_type, ret))\n\u001b[0;32m-> 1630\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m       raise RuntimeError(\n\u001b[1;32m   1632\u001b[0m           \u001b[0;34m\"%sConversion function %r for type %s returned incompatible \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/kerasenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;31m# Note: using the intern table directly here as this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# performance-sensitive in some models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datatype_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from seq2seq_util import test_predictions\n",
    "\n",
    "test_predictions(valid_samples[:100], inf_encoder, inf_decoder, encode_batch, translate_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions(train_samples[:100], inf_encoder, inf_decoder, encode_batch, translate_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export model in Core ML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_enc_in = Input(shape=(None, in_vocab_size), name=\"encoder_in\")\n",
    "coreml_enc_lstm = LSTM(latent_dim, return_state=True, name=\"encoder_lstm\")\n",
    "coreml_enc_out, _, _ = coreml_enc_lstm(coreml_enc_in)\n",
    "\n",
    "coreml_encoder_model = Model(coreml_enc_in, coreml_enc_out)\n",
    "coreml_encoder_model.output_layers = coreml_encoder_model._output_layers\n",
    "\n",
    "inf_encoder.save_weights(\"Es2EnCharEncoderWeights.h5\")\n",
    "coreml_encoder_model.load_weights(\"Es2EnCharEncoderWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 1.0.1 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:TensorFlow version 2.7.0 detected. Last version known to be fully compatible is 2.3.1 .\n",
      "WARNING:root:Keras version 2.7.0 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n",
      "2021-12-17 16:30:03.303550: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:03.317933: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 86 nodes (60), 99 edges (69), time = 2.505ms.\n",
      "  function_optimizer: Graph size after: 86 nodes (0), 99 edges (0), time = 1.137ms.\n",
      "Optimization results for grappler item: while_cond_1491\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "Optimization results for grappler item: while_body_1492\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-12-17 16:30:03.405950: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:03.443139: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 48 nodes (-6), 51 edges (-9), time = 22.447ms.\n",
      "  dependency_optimizer: Graph size after: 39 nodes (-9), 42 edges (-9), time = 0.885ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.082ms.\n",
      "  constant_folding: Graph size after: 39 nodes (0), 42 edges (0), time = 1.688ms.\n",
      "  dependency_optimizer: Graph size after: 39 nodes (0), 42 edges (0), time = 0.468ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.075ms.\n",
      "Optimization results for grappler item: while_cond_1491\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.387ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (-1), 3 edges (-1), time = 0.118ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.011ms.\n",
      "  constant_folding: Graph size after: 13 nodes (0), 3 edges (0), time = 0.153ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (0), 3 edges (0), time = 0.064ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.008ms.\n",
      "Optimization results for grappler item: while_body_1492\n",
      "  constant_folding: Graph size after: 50 nodes (0), 50 edges (0), time = 0.704ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (-6), 44 edges (-6), time = 0.289ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.025ms.\n",
      "  constant_folding: Graph size after: 44 nodes (0), 44 edges (0), time = 0.468ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (0), 44 edges (0), time = 0.236ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.027ms.\n",
      "\n",
      "2021-12-17 16:30:03.679595: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:03.695197: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 86 nodes (60), 99 edges (69), time = 3.609ms.\n",
      "  function_optimizer: Graph size after: 86 nodes (0), 99 edges (0), time = 1.429ms.\n",
      "Optimization results for grappler item: while_body_2093\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.004ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "Optimization results for grappler item: while_cond_2092\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-12-17 16:30:03.771591: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:03.806876: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 48 nodes (-6), 51 edges (-9), time = 15.964ms.\n",
      "  dependency_optimizer: Graph size after: 39 nodes (-9), 42 edges (-9), time = 1.26ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 1.904ms.\n",
      "  constant_folding: Graph size after: 39 nodes (0), 42 edges (0), time = 1.383ms.\n",
      "  dependency_optimizer: Graph size after: 39 nodes (0), 42 edges (0), time = 1.323ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.071ms.\n",
      "Optimization results for grappler item: while_body_2093\n",
      "  constant_folding: Graph size after: 50 nodes (0), 50 edges (0), time = 0.843ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (-6), 44 edges (-6), time = 0.373ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.025ms.\n",
      "  constant_folding: Graph size after: 44 nodes (0), 44 edges (0), time = 0.498ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (0), 44 edges (0), time = 0.242ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.022ms.\n",
      "Optimization results for grappler item: while_cond_2092\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.251ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (-1), 3 edges (-1), time = 0.074ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.008ms.\n",
      "  constant_folding: Graph size after: 13 nodes (0), 3 edges (0), time = 0.124ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (0), 3 edges (0), time = 0.059ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.007ms.\n",
      "\n",
      "Running TensorFlow Graph Passes: 100%|███████| 5/5 [00:00<00:00, 14.39 passes/s]\n",
      "Converting Frontend ==> MIL Ops:   0%|                 | 0/41 [00:00<?, ? ops/s]\n",
      "Converting Frontend ==> MIL Ops: 100%|█████| 14/14 [00:00<00:00, 16629.92 ops/s]\u001b[A\n",
      "\n",
      "Converting Frontend ==> MIL Ops:   0%|                 | 0/40 [00:00<?, ? ops/s]\u001b[AWARNING:root:Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting Frontend ==> MIL Ops: 100%|██████| 40/40 [00:00<00:00, 2454.28 ops/s]\n",
      "\n",
      "Converting Frontend ==> MIL Ops: 100%|█████| 14/14 [00:00<00:00, 13726.10 ops/s]\u001b[A\n",
      "\n",
      "Converting Frontend ==> MIL Ops:   0%|                 | 0/40 [00:00<?, ? ops/s]\u001b[AWARNING:root:Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting Frontend ==> MIL Ops: 100%|██████| 40/40 [00:00<00:00, 2536.97 ops/s]\n",
      "Converting Frontend ==> MIL Ops: 100%|███████| 41/41 [00:00<00:00, 462.60 ops/s]\n",
      "Running MIL Common passes: 100%|█████████| 33/33 [00:00<00:00, 3101.60 passes/s]\n",
      "Running MIL Clean up passes: 100%|█████████| 8/8 [00:00<00:00, 1758.99 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops:   0%|           | 0/66 [00:00<?, ? ops/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|███| 1/1 [00:00<00:00, 897.75 ops/s]\u001b[A\n",
      "\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 24/24 [00:00<00:00, 653.20 ops/s]\u001b[A\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 66/66 [00:00<00:00, 1218.61 ops/s\n"
     ]
    }
   ],
   "source": [
    "import coremltools\n",
    "\n",
    "coreml_encoder = coremltools.converters.convert(\n",
    "    coreml_encoder_model,\n",
    "    input_names=\"encodedSeq\",\n",
    "    output_names=\"ignored\")\n",
    "\n",
    "coreml_encoder.save(\"Es2EnCharEncoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_dec_in = Input(shape=(None, out_vocab_size))\n",
    "\n",
    "coreml_dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "coreml_dec_lstm_out, _, _ = coreml_dec_lstm(coreml_dec_in)\n",
    "coreml_dec_dense = Dense(out_vocab_size, activation=\"softmax\")\n",
    "coreml_dec_out = coreml_dec_dense(coreml_dec_lstm_out)\n",
    "\n",
    "coreml_decoder_model = Model(coreml_dec_in, coreml_dec_out)\n",
    "coreml_decoder_model.output_layers = coreml_decoder_model._output_layers\n",
    "\n",
    "inf_decoder.save_weights(\"Es2EnCharDecoderWeights.h5\")\n",
    "coreml_decoder_model.load_weights(\"Es2EnCharDecoderWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:30:14.829675: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:14.842193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 113 nodes (60), 133 edges (69), time = 2.493ms.\n",
      "  function_optimizer: Graph size after: 113 nodes (0), 133 edges (0), time = 1.399ms.\n",
      "Optimization results for grappler item: while_body_3422\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: while_cond_3421\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-12-17 16:30:14.935392: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:14.963415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 71 nodes (-8), 81 edges (-11), time = 12.136ms.\n",
      "  dependency_optimizer: Graph size after: 57 nodes (-14), 63 edges (-18), time = 1.272ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 1.235ms.\n",
      "  constant_folding: Graph size after: 57 nodes (0), 63 edges (0), time = 2.541ms.\n",
      "  dependency_optimizer: Graph size after: 57 nodes (0), 63 edges (0), time = 0.702ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.127ms.\n",
      "Optimization results for grappler item: while_body_3422\n",
      "  constant_folding: Graph size after: 50 nodes (0), 50 edges (0), time = 1.164ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (-6), 44 edges (-6), time = 0.341ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.032ms.\n",
      "  constant_folding: Graph size after: 44 nodes (0), 44 edges (0), time = 0.563ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (0), 44 edges (0), time = 0.263ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.033ms.\n",
      "Optimization results for grappler item: while_cond_3421\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.521ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (-1), 3 edges (-1), time = 0.121ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.039ms.\n",
      "  constant_folding: Graph size after: 13 nodes (0), 3 edges (0), time = 0.23ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (0), 3 edges (0), time = 0.076ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.008ms.\n",
      "\n",
      "2021-12-17 16:30:15.217323: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:15.233233: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 113 nodes (60), 133 edges (69), time = 2.862ms.\n",
      "  function_optimizer: Graph size after: 113 nodes (0), 133 edges (0), time = 1.502ms.\n",
      "Optimization results for grappler item: while_body_4106\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "Optimization results for grappler item: while_cond_4105\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-12-17 16:30:15.324390: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-12-17 16:30:15.351348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1149] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 71 nodes (-8), 81 edges (-11), time = 13.797ms.\n",
      "  dependency_optimizer: Graph size after: 57 nodes (-14), 63 edges (-18), time = 1.492ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.547ms.\n",
      "  constant_folding: Graph size after: 57 nodes (0), 63 edges (0), time = 1.656ms.\n",
      "  dependency_optimizer: Graph size after: 57 nodes (0), 63 edges (0), time = 0.665ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.118ms.\n",
      "Optimization results for grappler item: while_body_4106\n",
      "  constant_folding: Graph size after: 50 nodes (0), 50 edges (0), time = 0.743ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (-6), 44 edges (-6), time = 0.284ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.024ms.\n",
      "  constant_folding: Graph size after: 44 nodes (0), 44 edges (0), time = 0.494ms.\n",
      "  dependency_optimizer: Graph size after: 44 nodes (0), 44 edges (0), time = 0.237ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.029ms.\n",
      "Optimization results for grappler item: while_cond_4105\n",
      "  constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.271ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (-1), 3 edges (-1), time = 0.083ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.01ms.\n",
      "  constant_folding: Graph size after: 13 nodes (0), 3 edges (0), time = 0.138ms.\n",
      "  dependency_optimizer: Graph size after: 13 nodes (0), 3 edges (0), time = 0.069ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.007ms.\n",
      "\n",
      "Running TensorFlow Graph Passes: 100%|███████| 5/5 [00:00<00:00, 14.97 passes/s]\n",
      "Converting Frontend ==> MIL Ops:   0%|                 | 0/59 [00:00<?, ? ops/s]\n",
      "Converting Frontend ==> MIL Ops: 100%|█████| 14/14 [00:00<00:00, 21877.89 ops/s]\u001b[A\n",
      "\n",
      "Converting Frontend ==> MIL Ops:   0%|                 | 0/40 [00:00<?, ? ops/s]\u001b[AWARNING:root:Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting Frontend ==> MIL Ops: 100%|██████| 40/40 [00:00<00:00, 2517.17 ops/s]\n",
      "\n",
      "Converting Frontend ==> MIL Ops: 100%|█████| 14/14 [00:00<00:00, 15505.74 ops/s]\u001b[A\n",
      "\n",
      "Converting Frontend ==> MIL Ops:   0%|                 | 0/40 [00:00<?, ? ops/s]\u001b[AWARNING:root:Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting Frontend ==> MIL Ops: 100%|██████| 40/40 [00:00<00:00, 2311.71 ops/s]\n",
      "Converting Frontend ==> MIL Ops: 100%|███████| 59/59 [00:00<00:00, 257.72 ops/s]\n",
      "Running MIL Common passes: 100%|██████████| 33/33 [00:00<00:00, 812.41 passes/s]\n",
      "Running MIL Clean up passes: 100%|█████████| 8/8 [00:00<00:00, 1054.87 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops:   0%|           | 0/78 [00:00<?, ? ops/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██| 1/1 [00:00<00:00, 2651.27 ops/s]\u001b[A\n",
      "\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 24/24 [00:00<00:00, 493.30 ops/s]\u001b[A\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|█| 78/78 [00:00<00:00, 1138.34 ops/s\n"
     ]
    }
   ],
   "source": [
    "coreml_decoder = coremltools.converters.convert(\n",
    "    coreml_decoder_model,\n",
    "    input_names=\"encodedChar\",\n",
    "    output_names=\"nextCharProbs\")\n",
    "\n",
    "coreml_decoder.save(\"Es2EnCharDecoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert weights to 16bit floats. This shouldn't hurt performance much, if at all, and it reduces the app's download size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coremltools.models.neural_network import quantization_utils\n",
    "\n",
    "def convert_to_fp16(mlmodel_filename):\n",
    "    model_fp32 = coremltools.models.MLModel(mlmodel_filename)\n",
    "    spec_16bit = quantization_utils.quantize_weights(model_fp32, nbits=16)\n",
    "    coremltools.utils.save_spec(spec_16bit, f\"{mlmodel_filename}16Bit.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing using linear quantization\n",
      "Quantizing layer tf_make_list_0_condition_re_initialize\n",
      "Quantizing layer tf_make_list_0_condition\n",
      "Quantizing layer model_3/encoder_lstm/PartitionedCall/while_renamed\n",
      "Quantizing layer model_3/encoder_lstm/PartitionedCall/while/while_body_2093/while/MatMul_1\n",
      "Quantizing layer model_3/encoder_lstm/PartitionedCall/while/while_body_2093/while/MatMul\n",
      "Quantizing layer model_3/encoder_lstm/PartitionedCall/while_1_condition_re_initialize\n",
      "Quantizing layer model_3/encoder_lstm/PartitionedCall/while_1_condition\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'basename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g2/d875dk7s4xq8j03j59rzx0w40000gn/T/ipykernel_99443/1625645674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_to_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Es2EnCharEncoder.mlmodel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconvert_to_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Es2EnCharDecoder.mlmodel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g2/d875dk7s4xq8j03j59rzx0w40000gn/T/ipykernel_99443/1783711116.py\u001b[0m in \u001b[0;36mconvert_to_fp16\u001b[0;34m(mlmodel_filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_fp32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoremltools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mspec_16bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fp32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcoremltools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_16bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{basename}16Bit.mlmodel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'basename' is not defined"
     ]
    }
   ],
   "source": [
    "convert_to_fp16(\"Es2EnCharEncoder.mlmodel\")\n",
    "convert_to_fp16(\"Es2EnCharDecoder.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the maps so you can transform text to and from ints. You'll need them later in the iOS app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"esCharToInt.json\", \"w\") as f:\n",
    "    json.dump(in_token2int, f)\n",
    "with open(\"intToEnChar.json\", \"w\") as f:\n",
    "    json.dump(out_int2token, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
